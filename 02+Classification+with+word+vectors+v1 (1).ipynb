{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yamshee'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = \"/mnt/c/Users/yamshee.fatima/Desktop/r8-train-all-terms.txt\"\n",
    "test_data = \"/mnt/c/Users/yamshee.fatima/Desktop/r8-test-all-terms.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total examples 5485\n"
     ]
    }
   ],
   "source": [
    "X, y = [], []\n",
    "with open(train_data, \"r\") as infile:\n",
    "    for line in infile:\n",
    "        \n",
    "        label, text = line.split(\"\\t\")\n",
    "        #print(text)\n",
    "        #print(label)\n",
    "        X.append(text.split())\n",
    "        y.append(label)\n",
    "X, y = np.array(X), np.array(y)\n",
    "print (\"total examples %s\" % len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['champion', 'products', 'ch', 'approves', 'stock', 'split', 'champion', 'products', 'inc', 'said', 'its', 'board', 'of', 'directors', 'approved', 'a', 'two', 'for', 'one', 'stock', 'split', 'of', 'its', 'common', 'shares', 'for', 'shareholders', 'of', 'record', 'as', 'of', 'april', 'the', 'company', 'also', 'said', 'its', 'board', 'voted', 'to', 'recommend', 'to', 'shareholders', 'at', 'the', 'annual', 'meeting', 'april', 'an', 'increase', 'in', 'the', 'authorized', 'capital', 'stock', 'from', 'five', 'mln', 'to', 'mln', 'shares', 'reuter']\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "earn\n"
     ]
    }
   ],
   "source": [
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total examples 2189\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = [], []\n",
    "with open(test_data, \"r\") as infile:\n",
    "    for line in infile:\n",
    "        label, text = line.split(\"\\t\")\n",
    "        #print(text)\n",
    "        X_test.append(text.split())\n",
    "        y_test.append(label)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "print (\"total examples %s\" % len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5485,), (2189,))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['acq', 'crude', 'earn', 'grain', 'interest', 'money-fx', 'ship',\n",
       "        'trade'], dtype='<U8'),\n",
       " array([1596,  253, 2840,   41,  190,  206,  108,  251]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NB methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, reconstructring the string text for our vectorizers to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "champion products ch approves stock split champion products inc said its board of directors approved a two for one stock split of its common shares for shareholders of record as of april the company also said its board voted to recommend to shareholders at the annual meeting april an increase in the authorized capital stock from five mln to mln shares reuter\n",
      "\n",
      "asian exporters fear damage from u s japan rift mounting trade friction between the u s and japan has raised fears among many of asia s exporting nations that the row could inflict far reaching economic damage businessmen and officials said they told reuter correspondents in asian capitals a u s move against japan might boost protectionist sentiment in the u s and lead to curbs on american imports of their products but some exporters said that while the conflict would hurt them in the long run in the short term tokyo s loss might be their gain the u s has said it will impose mln dlrs of tariffs on imports of japanese electronics goods on april in retaliation for japan s alleged failure to stick to a pact not to sell semiconductors on world markets at below cost unofficial japanese estimates put the impact of the tariffs at billion dlrs and spokesmen for major electronics firms said they would virtually halt exports of products hit by the new taxes we wouldn t be able to do business said a spokesman for leading japanese electronics firm matsushita electric industrial co ltd mc t if the tariffs remain in place for any length of time beyond a few months it will mean the complete erosion of exports of goods subject to tariffs to the u s said tom murtha a stock analyst at the tokyo office of broker james capel and co in taiwan businessmen and officials are also worried we are aware of the seriousness of the u s threat against japan because it serves as a warning to us said a senior taiwanese trade official who asked not to be named taiwan had a trade trade surplus of billion dlrs last year pct of it with the u s the surplus helped swell taiwan s foreign exchange reserves to billion dlrs among the world s largest we must quickly open our markets remove trade barriers and cut import tariffs to allow imports of u s products if we want to defuse problems from possible u s retaliation said paul sheen chairman of textile exporters taiwan safe group a senior official of south korea s trade promotion association said the trade dispute between the u s and japan might also lead to pressure on south korea whose chief exports are similar to those of japan last year south korea had a trade surplus of billion dlrs with the u s up from billion dlrs in in malaysia trade officers and businessmen said tough curbs against japan might allow hard hit producers of semiconductors in third countries to expand their sales to the u s in hong kong where newspapers have alleged japan has been selling below cost semiconductors some electronics manufacturers share that view but other businessmen said such a short term commercial advantage would be outweighed by further u s pressure to block imports that is a very short term view said lawrence mills director general of the federation of hong kong industry if the whole purpose is to prevent imports one day it will be extended to other sources much more serious for hong kong is the disadvantage of action restraining trade he said the u s last year was hong kong s biggest export market accounting for over pct of domestically produced exports the australian government is awaiting the outcome of trade talks between the u s and japan with interest and concern industry minister john button said in canberra last friday this kind of deterioration in trade relations between two countries which are major trading partners of ours is a very serious matter button said he said australia s concerns centred on coal and beef australia s two largest exports to japan and also significant u s exports to that country meanwhile u s japanese diplomatic manoeuvres to solve the trade stand off continue japan s ruling liberal democratic party yesterday outlined a package of economic measures to boost the japanese economy the measures proposed include a large supplementary budget and record public works spending in the first half of the financial year they also call for stepped up spending as an emergency measure to stimulate the economy despite prime minister yasuhiro nakasone s avowed fiscal reform program deputy u s trade representative michael smith and makoto kuroda japan s deputy minister of international trade and industry miti are due to meet in washington this week in an effort to end the dispute reuter\n"
     ]
    }
   ],
   "source": [
    "X_text = [\" \".join(val) for val in X]\n",
    "print(X_text[0] + \"\\n\")\n",
    "X_test_text = [\" \".join(val) for val in X_test]\n",
    "print(X_test_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=5000, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words='english',max_features=5000)\n",
    "vect.fit(X_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('champion', 759),\n",
       " ('products', 3488),\n",
       " ('approves', 264),\n",
       " ('stock', 4337),\n",
       " ('split', 4273),\n",
       " ('said', 3974),\n",
       " ('board', 532),\n",
       " ('directors', 1327),\n",
       " ('approved', 263),\n",
       " ('common', 895)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the vocabulary\n",
    "list(vect.vocabulary_.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the vectorizer to our test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = vect.transform(X_text)\n",
    "X_test_transformed =vect.transform(X_test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Bernoulli NB first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.8736554238833182\n",
      "Test accuracy:  0.8688899040657835\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train_transformed,y)\n",
    "\n",
    "# predict class\n",
    "pred_train_ys = bnb.predict(X_train_transformed)\n",
    "pred_test_ys = bnb.predict(X_test_transformed)\n",
    "\n",
    "# accuracy\n",
    "print(\"Train accuracy: \", accuracy_score(y, pred_train_ys))\n",
    "print(\"Test accuracy: \", accuracy_score(y_test, pred_test_ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Multinomial NB\n",
    " - We expect this to work very well, giving high performance in accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.968094804010939\n",
      "Test accuracy:  0.9657377798081316\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "#fit on training data\n",
    "mnb.fit(X_train_transformed, y)\n",
    "\n",
    "# predict class\n",
    "pred_train_ys = mnb.predict(X_train_transformed)\n",
    "pred_test_ys = mnb.predict(X_test_transformed)\n",
    "\n",
    "# accuracy\n",
    "print(\"Train accuracy: \", accuracy_score(y, pred_train_ys))\n",
    "print(\"Test accuracy: \", accuracy_score(y_test, pred_test_ys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - As expected, this performed really well\n",
    " - Remember that we used 5000 features!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using our word embeddings approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We have two options here:\n",
    " \n",
    "1. Use pre-trained word vectors (Glove)\n",
    "\n",
    "2. Train our own vectors\n",
    "\n",
    "We'll explore both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the pre-trained GloVe word vectors\n",
    "\n",
    "\n",
    "To download the pre-trained GloVe vectors, please first create an account in Kaggle and then download the following data file GloVe vectors.\n",
    "https://www.kaggle.com/incorpes/glove6b200d#glove.6B.200d.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 200)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file = 'glove.6B.200d.txt'\n",
    "word2vec_output_file = 'glove.6B.200d.w2vformat.txt'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7fed9dcc8208>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "glove_model = KeyedVectors.load_word2vec_format(\"glove.6B.200d.w2vformat.txt\", binary=False)\n",
    "glove_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence vector by averaging word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_vec(sent):\n",
    "    wv_res = np.zeros(glove_model.vector_size)\n",
    "    ctr = 1\n",
    "    for w in sent:\n",
    "        if w in glove_model:\n",
    "            ctr += 1\n",
    "            wv_res += glove_model[w]\n",
    "    wv_res = wv_res/ctr\n",
    "    #return (wv_res, ctr)\n",
    "    return wv_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_vecs = []\n",
    "for doc in X:  \n",
    "    doc_words = [term for term in doc if term not in stop_words]\n",
    "    train_doc_vecs.append(sent_vec(doc_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc_vecs = []\n",
    "for doc in X_test:    \n",
    "    doc_words = [term for term in doc if term not in stop_words]\n",
    "    test_doc_vecs.append(sent_vec(doc_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a predictive model on the averaged word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a 'simple' logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamshee/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/yamshee/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=3.5, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l1', random_state=42, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(penalty=\"l1\", random_state=42, C = 3.5)\n",
    "logreg.fit(train_doc_vecs,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.9781221513217867\n",
      "Test accuracy:  0.9680219278209228\n"
     ]
    }
   ],
   "source": [
    "pred_train_ys = logreg.predict(train_doc_vecs)\n",
    "pred_test_ys = logreg.predict(test_doc_vecs)\n",
    "print(\"Train accuracy: \", accuracy_score(pred_train_ys, y))\n",
    "print(\"Test accuracy: \", accuracy_score(pred_test_ys, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our own wordvectors on the data\n",
    "We'll create a combined text file to train our word vectors - more data is better. Although in this case we would still have just 7.7K instances to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7674"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_comb = np.concatenate([X,X_test])\n",
    "len(X_comb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['champion', 'products', 'ch', 'approves', 'stock', 'split', 'champion', 'products', 'inc', 'said', 'its', 'board', 'of', 'directors', 'approved', 'a', 'two', 'for', 'one', 'stock', 'split', 'of', 'its', 'common', 'shares', 'for', 'shareholders', 'of', 'record', 'as', 'of', 'april', 'the', 'company', 'also', 'said', 'its', 'board', 'voted', 'to', 'recommend', 'to', 'shareholders', 'at', 'the', 'annual', 'meeting', 'april', 'an', 'increase', 'in', 'the', 'authorized', 'capital', 'stock', 'from', 'five', 'mln', 'to', 'mln', 'shares', 'reuter']\n"
     ]
    }
   ],
   "source": [
    "print(X_comb[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = word2vec.Word2Vec(X_comb, window=2, min_count=2, sg = 1, size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamshee/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('near', 0.8351353406906128),\n",
       " ('positive', 0.8052511215209961),\n",
       " ('prospects', 0.7987924218177795),\n",
       " ('foreseeable', 0.7810018062591553),\n",
       " ('significant', 0.7740465402603149)]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(\"future\", topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence vectors by averaging vectors for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_vec_w2v(sent):\n",
    "    wv_res = np.zeros(w2v.vector_size)\n",
    "    ctr = 1\n",
    "    for w in sent:\n",
    "        if w in w2v:\n",
    "            ctr += 1\n",
    "            wv_res += w2v[w]\n",
    "    wv_res = wv_res/ctr\n",
    "    return wv_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the sentence vectors for the test and train sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamshee/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \"\"\"\n",
      "/home/yamshee/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "train_doc_vecs = []\n",
    "for doc in X:    \n",
    "    doc_words = [term for term in doc if term not in stop_words]\n",
    "    train_doc_vecs.append(sent_vec_w2v(doc_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamshee/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  \"\"\"\n",
      "/home/yamshee/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "test_doc_vecs = []\n",
    "for doc in X_test:    \n",
    "    doc_words = [term for term in doc if term not in stop_words]\n",
    "    test_doc_vecs.append(sent_vec_w2v(doc_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(penalty=\"l1\", random_state=42, C = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yamshee/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/yamshee/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=8, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l1', random_state=42, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(train_doc_vecs,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.9719234275296262\n",
      "Test accuracy:  0.962996802192782\n"
     ]
    }
   ],
   "source": [
    "pred_train_ys = logreg.predict(train_doc_vecs)\n",
    "pred_test_ys = logreg.predict(test_doc_vecs)\n",
    "print(\"Train accuracy: \", accuracy_score(pred_train_ys, y))\n",
    "print(\"Test accuracy: \", accuracy_score(pred_test_ys, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['earn', 'acq', 'earn', ..., 'earn', 'money-fx', 'ship'],\n",
       "      dtype='<U8')"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_train_ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
